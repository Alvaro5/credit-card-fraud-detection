{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f363c5cf94bfff3",
   "metadata": {},
   "source": [
    "# Project Step 1\n",
    "By Leo Winter, Yoann Sublet, Kellian VERVAELE--KLEIN\n",
    "\n",
    "1. Descriptive analysis of your data.\n",
    "2. Implementation of the necessary pre-processing.\n",
    "3. Formalisation of the problem.\n",
    "4. Selection of a baseline model and implementation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d2f5d5a365b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670466086334cce1",
   "metadata": {},
   "source": [
    "# 1) Descriptive analysis of the Credit Card Fraud Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d758f0efc78e458",
   "metadata": {},
   "source": [
    "### 1. Informations on the dataset\n",
    "\n",
    "Dataset description (from Kaggle):\n",
    "* The dataset contains transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "* It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, the original features and background information about the data cannot be provided. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n",
    "Explanation of each known feature:\n",
    "- V1 to V28: anonymized features obtained through PCA transformation\n",
    "- Time: Seconds elapsed between this transaction and the first transaction in the dataset\n",
    "- Amount: Transaction amount\n",
    "- Class: target variable, either 1 (fraud) or 0 (legitimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd537cc4be1e8e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"creditcard.csv\", sep=\",\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c8783107b0e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"Number of Rows: {df.shape[0]:,}\")\n",
    "print(f\"Number of Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2678d79a14392c6a",
   "metadata": {},
   "source": [
    "- 284 807 entries, without no missing data (0 NaN or null rows)\n",
    "- Data types: float64 for all features a part for the class which is int64 since its binary (either 0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c311d737e09b694",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2409847987991d26",
   "metadata": {},
   "source": [
    "#### Statistical summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d84180ab9e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e14440be8169f18",
   "metadata": {},
   "source": [
    "### 2. Visualizations of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b146873c87728",
   "metadata": {},
   "source": [
    "Visualization of V1 and V2 over Time with amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63e290aa36fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = px.scatter(df, x=\"V1\", y=\"V2\", size=\"Amount\", color=df[\"Class\"].astype(str),\n",
    "                   color_discrete_map={'1': \"red\", '0': \"green\"},\n",
    "                   title=\"Repartition of transactions in function of V1 and V2\")\n",
    "fig1.update_traces(marker=dict(showscale=False))\n",
    "fig1.update_layout(legend_title_text=\"Transaction type\")\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d9a1bed85d228",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = px.scatter(df[df[\"Class\"]==1], x=\"V1\", y=\"V2\", size=\"Amount\", color=\"Class\",\n",
    "                  title=\"Repartition of fraud transactions in function of V1 and V2\")\n",
    "fig2.update_traces(marker=dict(showscale=False))\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e4eaf2b1fce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fig3 = df[(df[\"V2\"]>-10) & (df[\"V1\"]>-31)]\n",
    "fig3 = px.scatter(df_fig3, x=\"V1\", y=\"V2\", size=\"Amount\", color=df_fig3[\"Class\"].astype(str),\n",
    "                  color_discrete_map={'1': \"red\", '0': \"green\"},\n",
    "                  title=\"Zoomed repartition of transactions in function of V1 and V2\")\n",
    "fig3.update_traces(marker=dict(showscale=False))\n",
    "fig3.update_layout(legend_title_text=\"Transaction type\")\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995a0b6f3019d085",
   "metadata": {},
   "source": [
    "# 2) Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf958ce957d4c8",
   "metadata": {},
   "source": [
    "#### Checking for duplicates and dropping them if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ce196a76c44327",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df.copy()\n",
    "# Check for duplicates\n",
    "duplicates = df_processed.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    df_processed = df_processed.drop_duplicates()\n",
    "    print(f\"New Dataset shape: {df_processed.shape}\")\n",
    "else:\n",
    "    print(\"No duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f7518bcf7bac8c",
   "metadata": {},
   "source": [
    "#### Separation of features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65111dd83212ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_processed.drop(\"Class\", axis=1)\n",
    "y = df_processed[\"Class\"]\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79b1d7f217c666",
   "metadata": {},
   "source": [
    "#### Scaling with StandardScaler: only for the features Time and Amount.\n",
    "Features V1 to V28 are already scaled (PCA transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd9e84531caec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61357960319b8ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling of Time and Amount features using the StandardScaler\n",
    "X['Time_scaled'] = scaler.fit_transform(X[['Time']])\n",
    "X['Amount_scaled'] = scaler.fit_transform(X[['Amount']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8618b41a404b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original Time and Amount columns\n",
    "X = X.drop(['Time', 'Amount'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511cf94e903c0d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[['Time_scaled', 'Amount_scaled']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6efc5364540f8f",
   "metadata": {},
   "source": [
    "#### Splitting the dataset into train and test sets\n",
    "We use the stratify parameter of train_test_split to balance the target since our dataset is very imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578817370c55def",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42, test_size=0.2, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]:,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fba201f",
   "metadata": {},
   "source": [
    "# 3) Problem Formalization\n",
    "\n",
    "In this section, we formally define the machine learning problem, our target, the metrics we will use to measure success, and any constraints we need to consider.\n",
    "\n",
    "#### 3.1. Type of Problem\n",
    "\n",
    "This is a binary classification problem.  \n",
    "For every transaction given as input, our model must decide which of two possible categories it belongs to:\n",
    "\n",
    "- Class 0: Normal (a legitimate transaction)  \n",
    "- Class 1: Fraud (a fraudulent transaction)\n",
    "\n",
    "The output of our model will be a prediction of either 0 or 1.\n",
    "\n",
    "#### 3.2. Target Variable\n",
    "\n",
    "The target variable that we are trying to predict is the Class column in the dataset.\n",
    "\n",
    "#### 3.3. Evaluation Metrics\n",
    "\n",
    "This is the most critical part of our problem formalization.  \n",
    "The dataset is severely imbalanced, with \"Fraud\" (Class 1) transactions representing only about 0.172% of the total.\n",
    "\n",
    "Because of this, Accuracy is a useless metric.  \n",
    "\n",
    "##### Confusion Matrix\n",
    "\n",
    "|                      | Predicted Fraud (1) | Predicted Normal (0) |\n",
    "|----------------------|--------------------------|---------------------------|\n",
    "| Actual Fraud (1) | True Positive (TP)     | False Negative (FN)     |\n",
    "| Actual Normal (0)| False Positive (FP)    | True Negative (TN)      |\n",
    "\n",
    "- True Positives (TP): The model predicts \"Fraud,\" and it is a \"Fraud.\"  \n",
    "- True Negatives (TN): The model predicts \"Normal,\" and it is \"Normal.\"\n",
    "- False Positives (FP) / Type I Error: The model predicts \"Fraud,\" but it was \"Normal.\"\n",
    "- False Negatives (FN) / Type II Error: The model predicts \"Normal,\" but it was a \"Fraud.\"\n",
    "\n",
    "\n",
    "##### Recall\n",
    "\n",
    "Recall = TP/(TP + FN)\n",
    "\n",
    "This is our most important metric.  \n",
    "We want to maximize this, as False Negatives are the most costly error.\n",
    "\n",
    "##### Precision\n",
    "\n",
    "Precision = TP/(TP + FP)\n",
    "\n",
    "If precision is low, we generate too many False Positives.\n",
    "\n",
    "##### F1-Score\n",
    "\n",
    "F1 = (2 * Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "It provides a single score that balances precision and recall, which is useful for comparing models.\n",
    "\n",
    "##### Area Under the Precision-Recall Curve (AUC-PR)\n",
    "\n",
    "Unlike the standard AUC-ROC, the AUC-PR is much more informative on heavily imbalanced datasets.  \n",
    "It provides a summary of the model's performance across all possible decision thresholds, focusing on the trade-off between Precision and Recall.\n",
    "\n",
    "#### 3.4. Constraints\n",
    "\n",
    "- Cost of Errors:\n",
    "  The cost is asymmetric.  \n",
    "  A False Negative (FN) (missing a fraud) has a direct and high financial cost.  \n",
    "  A False Positive (FP) (flagging a normal transaction) has a lower, but non-zero, operational cost (customer support, user friction).  \n",
    "  Our model must prioritize minimizing FNs (maximizing Recall).\n",
    "\n",
    "- Interpretability:\n",
    "  For regulatory and diagnostic purposes, it's highly desirable to understand why a model flagged a transaction as fraudulent.  \n",
    "  This suggests that \"black box\" models might be less ideal than simpler, interpretable models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbfdde306cb0f5",
   "metadata": {},
   "source": [
    "# 4) Implementation of a baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c984e51fc6d719aa",
   "metadata": {},
   "source": [
    "We create a Decision Tree with the DecisionTreeClassifier class from Scikit-learn using a random state of 42 and applying the class_weight parameter to 'balanced' since our dataset is heavily unbalanced.\n",
    "\n",
    "From the Scikit-learn documentation: The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a60486c418568",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238284d2c69f59a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
