{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d60571",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kaggle'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m project_root = os.path.abspath(os.path.join(os.getcwd(), \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     29\u001b[39m sys.path.append(os.path.join(project_root, \u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload_data\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m download_creditcard_data, load_creditcard_df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leoma\\Documents\\ESILV\\A4 S1\\Machine Learning\\projet\\credit-card-fraud-detection\\src\\data\\load_data.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkaggle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkaggle_api_extended\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KaggleApi\n\u001b[32m      5\u001b[39m PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(\u001b[34m__file__\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      7\u001b[39m DATA_DIR = os.path.join(PROJECT_ROOT, \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'kaggle'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier\n",
    "\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# add src to path so we can import our data module\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(os.path.join(project_root, \"src\"))\n",
    "\n",
    "from data.load_data import download_creditcard_data, load_creditcard_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ca44d",
   "metadata": {},
   "source": [
    "# Project step 2: Grid search and ensembles on credit card fraud\n",
    "\n",
    "In this notebook we apply grid search to our baseline models and we build ensemble models.\n",
    "We work on the same credit card fraud dataset as in project step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we download the data (if already downloaded this does nothing important)\n",
    "download_creditcard_data()\n",
    "df = load_creditcard_df()\n",
    "\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "X = df.drop(\"Class\", axis=1)\n",
    "y = df[\"Class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42,)\n",
    "\n",
    "# we check class balance on train and test\n",
    "print(\"Train class distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nTest class distribution:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5761c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we scale only Time and Amount and keep PCA components as they are\n",
    "numeric_to_scale = [\"Time\", \"Amount\"]\n",
    "other_features = [col for col in X.columns if col not in numeric_to_scale + [\"Class\"]]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"scale_time_amount\", StandardScaler(), numeric_to_scale),\n",
    "        (\"pass_others\", \"passthrough\", other_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3b5ec",
   "metadata": {},
   "source": [
    "## 1. Grid search on baseline models\n",
    "\n",
    "We tune two baseline models:\n",
    "- a Decision Tree with class weights for imbalance\n",
    "- a Logistic Regression with class weights\n",
    "\n",
    "We use F1 score for the fraud class as the main objective in the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c023bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for decision tree\n",
    "dt_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", DecisionTreeClassifier(class_weight=\"balanced\", random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid_dt = {\n",
    "    \"model__criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"model__max_depth\": [3, 5, 7, 9, None],\n",
    "    \"model__min_samples_split\": [2, 5, 10],\n",
    "    \"model__min_samples_leaf\": [1, 2, 4],\n",
    "}\n",
    "\n",
    "grid_dt = GridSearchCV(\n",
    "    estimator=dt_pipe,\n",
    "    param_grid=param_grid_dt,\n",
    "    scoring=\"f1\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Decision Tree params:\")\n",
    "print(grid_dt.best_params_)\n",
    "print(f\"Best CV F1 score: {grid_dt.best_score_:.4f}\")\n",
    "\n",
    "best_dt = grid_dt.best_estimator_\n",
    "\n",
    "y_pred_dt = best_dt.predict(X_test)\n",
    "y_proba_dt = best_dt.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nDecision Tree classification report (test):\")\n",
    "print(classification_report(y_test, y_pred_dt, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682038f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Grid search for Logistic Regression\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m lr_pipe = \u001b[43mPipeline\u001b[49m(\n\u001b[32m      3\u001b[39m     steps=[\n\u001b[32m      4\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33mpreprocess\u001b[39m\u001b[33m\"\u001b[39m, preprocessor),\n\u001b[32m      5\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, LogisticRegression(class_weight=\u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m, max_iter=\u001b[32m1000\u001b[39m)),\n\u001b[32m      6\u001b[39m     ]\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m param_grid_lr = {\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel__C\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m0.01\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m10\u001b[39m],\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel__penalty\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33ml2\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel__solver\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mliblinear\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlbfgs\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     13\u001b[39m }\n\u001b[32m     15\u001b[39m grid_lr = GridSearchCV(\n\u001b[32m     16\u001b[39m     estimator=lr_pipe,\n\u001b[32m     17\u001b[39m     param_grid=param_grid_lr,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     verbose=\u001b[32m1\u001b[39m,\n\u001b[32m     22\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Grid search for Logistic Regression\n",
    "lr_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", LogisticRegression(class_weight=\"balanced\", max_iter=1000)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid_lr = {\n",
    "    \"model__C\": [0.01, 0.1, 1, 10],\n",
    "    \"model__penalty\": [\"l2\"],\n",
    "    \"model__solver\": [\"liblinear\", \"lbfgs\"],\n",
    "}\n",
    "\n",
    "grid_lr = GridSearchCV(\n",
    "    estimator=lr_pipe,\n",
    "    param_grid=param_grid_lr,\n",
    "    scoring=\"f1\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Logistic Regression params:\")\n",
    "print(grid_lr.best_params_)\n",
    "print(f\"Best CV F1 score: {grid_lr.best_score_:.4f}\")\n",
    "\n",
    "best_lr = grid_lr.best_estimator_\n",
    "\n",
    "y_pred_lr = best_lr.predict(X_test)\n",
    "y_proba_lr = best_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nLogistic Regression classification report (test):\")\n",
    "print(classification_report(y_test, y_pred_lr, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
