{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    roc_auc_score,\n    confusion_matrix,\n    classification_report,\n)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nproject_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\nsys.path.append(project_root)\n\nfrom data.load_data import download_creditcard_data, load_creditcard_df"
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Project step 4: XGBoost on credit card fraud detection\n",
    "\n",
    "In this notebook we use XGBoost, a gradient boosting algorithm that builds trees sequentially. Each new tree tries to fix the mistakes of the previous ones. Its known for being fast and accurate on tabular data like ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_creditcard_data()\n",
    "df = load_creditcard_df()\n",
    "\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Fraud percentage: {df['Class'].mean() * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Class\", axis=1)\n",
    "y = df[\"Class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Fraud cases: {y_train.sum()}\")\n",
    "print(f\"Test size: {len(X_test)}, Fraud cases: {y_test.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessor",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_to_scale = [\"Time\", \"Amount\"]\n",
    "other_features = [col for col in X.columns if col not in numeric_to_scale]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"scale_time_amount\", StandardScaler(), numeric_to_scale),\n",
    "        (\"pass_others\", \"passthrough\", other_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline_section",
   "metadata": {},
   "source": [
    "## 1. Baseline XGBoost\n",
    "\n",
    "XGBoost handles imbalanced data with the scale_pos_weight parameter. We set it to the ratio of negative to positive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline_xgb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate class imbalance ratio\n",
    "scale_pos = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"scale_pos_weight: {scale_pos:.2f}\")\n",
    "\n",
    "baseline_xgb = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            scale_pos_weight=scale_pos,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            eval_metric=\"logloss\",\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline_xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_baseline = baseline_xgb.predict(X_test)\n",
    "y_proba_baseline = baseline_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nBaseline XGBoost results:\")\n",
    "print(classification_report(y_test, y_pred_baseline, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tuning_section",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter tuning\n",
    "\n",
    "XGBoost has many parameters. The most important ones are:\n",
    "- n_estimators: number of trees\n",
    "- max_depth: depth of each tree\n",
    "- learning_rate: how much each tree contributes\n",
    "- subsample: fraction of samples used per tree\n",
    "- colsample_bytree: fraction of features used per tree\n",
    "\n",
    "We use RandomizedSearchCV to explore different combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tuning_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "\n",
    "xgb_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", XGBClassifier(\n",
    "            scale_pos_weight=scale_pos,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            eval_metric=\"logloss\",\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_distributions = {\n",
    "    \"model__n_estimators\": randint(50, 200),\n",
    "    \"model__max_depth\": randint(3, 10),\n",
    "    \"model__learning_rate\": uniform(0.01, 0.29),\n",
    "    \"model__subsample\": uniform(0.6, 0.4),\n",
    "    \"model__colsample_bytree\": uniform(0.6, 0.4),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tuning_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_pipe,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,\n",
    "    scoring=\"f1\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest CV F1 score: {random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best_model_eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "y_pred_tuned = best_xgb.predict(X_test)\n",
    "y_proba_tuned = best_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Tuned XGBoost results:\")\n",
    "print(classification_report(y_test, y_pred_tuned, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison_section",
   "metadata": {},
   "source": [
    "## 3. Comparison: baseline vs tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, y_proba, name):\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_proba),\n",
    "    }\n",
    "\n",
    "results = []\n",
    "results.append(compute_metrics(y_test, y_pred_baseline, y_proba_baseline, \"XGB Baseline\"))\n",
    "results.append(compute_metrics(y_test, y_pred_tuned, y_proba_tuned, \"XGB Tuned\"))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion_matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "models_preds = {\n",
    "    \"XGB Baseline\": y_pred_baseline,\n",
    "    \"XGB Tuned\": y_pred_tuned,\n",
    "}\n",
    "\n",
    "for ax, (name, y_pred) in zip(axes, models_preds.items()):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1_barplot",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(results_df[\"model\"], results_df[\"f1\"], color=[\"steelblue\", \"green\"])\n",
    "plt.ylabel(\"F1 score (fraud class)\")\n",
    "plt.title(\"F1 scores comparison\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_importance_section",
   "metadata": {},
   "source": [
    "## 4. Feature importance\n",
    "\n",
    "XGBoost also provides feature importances, lets see which features matter most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = best_xgb.named_steps[\"model\"]\n",
    "importances = xgb_model.feature_importances_\n",
    "\n",
    "feature_names = numeric_to_scale + other_features\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(importance_df[\"feature\"][:15][::-1], importance_df[\"importance\"][:15][::-1])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Top 15 most important features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 features:\")\n",
    "print(importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": "## 5. Conclusion\n\nXGBoost delivers strong results on our fraud detection problem. Both baseline and tuned models achieve an F1 score around 0.85-0.86, which is better than what we got with Random Forest (0.83).\n\nLooking at the confusion matrices, the tuned model catches 74 out of 95 frauds (78% recall) while only generating 3 false alarms. The baseline is almost identical with 73 true positives. This shows that XGBoost with scale_pos_weight already works well out of the box, and tuning only gives a small improvement.\n\nThe precision is excellent at 96%, meaning when the model flags a transaction as fraud, its almost always right. This is important in practice because too many false alerts would annoy customers and overload the fraud team.\n\nCompared to our previous models, XGBoost gives the best balance between catching frauds and avoiding false positives. The gradient boosting approach where each tree corrects the errors of previous ones seems to work well for this type of tabular data."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}